{"cells":[{"cell_type":"markdown","metadata":{"id":"eLLMYew9GzKK"},"source":["\n","# Machine Learning Methods Overview\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-24T19:42:10.881014Z","start_time":"2023-05-24T19:42:09.690490Z"},"id":"mwDsLouwGzKO"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"GdKzhwXqGzKP"},"source":["# Supervised Learning"]},{"cell_type":"markdown","metadata":{"id":"SniixqQJGzKP"},"source":[">Supervised learning algorithms are a class of machine learning algorithms that\n","use previously-labeled data to learn its features, so they can classify similar but unlabeled data. Let's use an example to understand this concept better."]},{"cell_type":"markdown","metadata":{"id":"WquBLsR7GzKQ"},"source":["### Preprocessing Real Estate Data"]},{"cell_type":"markdown","metadata":{"id":"YOsj3EPxGzKQ"},"source":[">Let's first read in a csv file using the `pandas.read_csv` function.\n","\n","- `read_csv` : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEOMX9koGzKQ"},"outputs":[],"source":["df = pd.read_csv(\"https://ist691.s3.amazonaws.com/real-estate.csv\")\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"x1q0a6x5GzKS"},"source":["> Dropping irrelevant columns: The `No` columns is simply the row number, which is unique for each row, and `X1 transction date` is not needed.\n","\n","- `drop` : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRT0nLjEGzKT"},"outputs":[],"source":["# drop takes first argument as column name and other optional arguments\n","# axis = 1 means dropping columns\n","# inplace = True means making changing in the existing dataframe\n","df.drop('No', axis = 1, inplace = True)\n","df.drop('X1 transaction date', axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xnh6B90GzKT"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"nvaZ7KQAGzKU"},"source":["> Using the `train_test_split` function in sklearn's `model_selection` module to divide our into two sets, one for training and other for testing.\n","\n","- `train_test_split` : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","\n","- `iloc` : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1q6WaMkMGzKU"},"outputs":[],"source":["# for splitting the data into train and test\n","from sklearn.model_selection import train_test_split\n","# iloc is used to select columns and rows\n","# the first set of arguments is for the rows and second is for the columns\n","X = df.iloc[:,0:-1]\n","Y = df.iloc[:,-1]\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.05, random_state = 0)"]},{"cell_type":"markdown","metadata":{"id":"XNvs6bjXGzKV"},"source":["### Linear Regression\n","\n",">For linear regression, we will use real estate data for the prediction of house prices. The data contains 6 features: transaction date, age of the house, distance to the nearest MRT station, number of convenience stores, and its latitude and longitude.\n","\n","- `LinearRegression` : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aqr2yvh3GzKV"},"outputs":[],"source":["# for the linear regression model\n","from sklearn.linear_model import LinearRegression\n","\n","reg = LinearRegression()\n","# the model.fit function trains the model with the training set passed\n","reg.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"yoR19fI9GzKW"},"source":[">Predicting the prices of the house on test data by using the trained model, then printing the coeficients of the linear regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhTj9t61GzKW"},"outputs":[],"source":["# model.predict will predict the output of the data provided as argument\n","lr_pred = reg.predict(X_test)\n","print(set(zip(reg.feature_names_in_, reg.coef_)))"]},{"cell_type":"markdown","metadata":{"id":"q2CeBbrbGzKW"},"source":[">In a linear-regression algorithm, the goal is to minimize a cost function. A popular cost function is **Mean Square Error (MSE)**, where we take the square of the difference between the expected value and the predicted value. The average over all the input examples gives us the mean error of the algorithm and represents the cost function.\n","\n","- `mean_squared_error` - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-dhSaPPGzKX"},"outputs":[],"source":["# importing the mean_squared_error function\n","from sklearn.metrics import mean_squared_error\n","mean_squared_error(lr_pred, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7covi7vHGzKX"},"outputs":[],"source":["fig, ax = plt.subplots()\n","ax.plot([0,1],[0,1], transform = ax.transAxes)\n","\n","plt.scatter(lr_pred, y_test)\n","plt.xlabel('Predicted')\n","plt.ylabel('Observed')\n","plt.title('Predicting House Prices with Linear Regression')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bfq6pxHsGzKX"},"source":["### Preprocessing the Iris Dataset\n","\n","This is perhaps the best known data set to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n","\n","Predicted attribute: class of iris plant.\n","\n","Attribute Information:\n","\n","1. sepal length in cm\n","2. sepal width in cm\n","3. petal length in cm\n","4. petal width in cm\n","5. class: Iris Setosa, Iris Versicolour, Iris Virginica\n","\n","- `plot_iris_dataset` : https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n","\n","- `StandardScaler` : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQksVjh4GzKY"},"outputs":[],"source":["# loading the iris dataset from sklearn dataset class\n","from sklearn import datasets\n","iris = datasets.load_iris()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLhSM2oUGzKY"},"outputs":[],"source":["# using train test split to divide the iris dataset into training and testing group\n","X_train, X_test, y_train, y_test = train_test_split(iris.data,\n","                                                    iris.target,\n","                                                    test_size = .3,\n","                                                    random_state = 0)\n","\n","print('Training set has {} samples and testing set has {} samples.'\\\n","      .format(X_train.shape[0], X_test.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qd8VZE_nGzKY"},"outputs":[],"source":["# using sklearn StandardScaler to standarize the features of training and test.\n","# it will scale our data to unit variance\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","sc.fit(X_train)\n","\n","# using StandardScaler transform method to transform data\n","# more details can be found in the documentation from the link above\n","X_train_std = sc.transform(X_train)\n","X_test_std = sc.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"FeN8XT5RGzKY"},"source":["### Logistic Regression\n","\n","- `LogisticRegression` : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6qOOT82GzKZ"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression(random_state = 0)\\\n","  .fit(X_train_std, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0t3mNp4GzKZ"},"outputs":[],"source":["# test set predictions using LogisticRegression model\n","clf.predict(X_test_std)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcN1zVuTGzKZ"},"outputs":[],"source":["# actual values\n","y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ct9G2hHKGzKZ"},"outputs":[],"source":["# compare predictions to actual values for accuracy of predictions\n","clf.score(X_test_std, y_test)"]},{"cell_type":"markdown","metadata":{"id":"Xc1qs80KGzKZ"},"source":["### Support Vector Machine\n","\n","- `SVM` : https://scikit-learn.org/stable/modules/classes.html?highlight=svm#module-sklearn.svm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGp0aReBGzKa"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","# using a support vector classifier (SVC) with a radial basis function kernal\n","svm = SVC(kernel = 'rbf', random_state = 0, gamma = .10, C = 1.0)\n","svm.fit(X_train_std, y_train)\n","\n","print('The accuracy of the svm classifier on training data is {:.2f} out of 1'\\\n","      .format(svm.score(X_train_std, y_train)))\n","print('The accuracy of the svm classifier on test data is {:.2f} out of 1'\\\n","      .format(svm.score(X_test_std, y_test)))"]},{"cell_type":"markdown","metadata":{"id":"Xvp8yFZgGzKa"},"source":["### Decision Tree\n","\n","- `DecisionTreeClassifier` : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier\n","\n","- `plot_tree` : https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html?highlight=plot_tree#sklearn.tree.plot_tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qud3OwHqGzKa"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier, plot_tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ar-0NcLnGzKb"},"outputs":[],"source":["dt = DecisionTreeClassifier().fit(iris.data, iris.target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZKL_DT_GzKb"},"outputs":[],"source":["plt.figure(figsize = (16, 8))\n","# plot the decision tree, showing the decisive values and the improvements in Gini impurity\n","plot_tree(dt, filled = True)\n","# display the tree plot figure\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"a8AD5DHvGzKb"},"source":["### Preprocessing Cancer Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BQo5X4yGzKb"},"outputs":[],"source":["# using the pandas library to read a csv file\n","df = pd.read_csv(\"https://ist691.s3.amazonaws.com/cancer.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxYKbb-mGzKc"},"outputs":[],"source":["df.drop(['id'], axis = 1, inplace = True)\n","df.drop(['Unnamed: 32'], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iE6cZQeGGzKc"},"outputs":[],"source":["df['diagnosis'] = [1 if i == 'M' else 0 for i in df.diagnosis]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZXBAo4xGzKc"},"outputs":[],"source":["df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yEEKiNGGzKc"},"outputs":[],"source":["x = df.drop(['diagnosis'], axis = 1)\n","y = df.diagnosis.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5zVvp9yGzKd"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .3, random_state = 0)\n","print('Training set has {} samples and testing set has {} samples.'\\\n","      .format(X_train.shape[0], X_test.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brlF1js4GzKd"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","sc.fit(X_train)\n","\n","X_train_std = sc.transform(X_train)\n","X_test_std = sc.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"iKQZCtP6GzKd"},"source":["### Gaussian Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"j2SF7HLIGzKd"},"source":[" - `GaussianNB` : https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8JOwUqOGzKd"},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","nb = GaussianNB()\n","nb.fit(X_train_std, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4aRzSUxGzKd"},"outputs":[],"source":["print('Naive Bayes score: ',nb.score(X_test_std, y_test))"]},{"cell_type":"markdown","metadata":{"id":"MA__GjNLNPO-"},"source":["# Unsupervised Learning"]},{"cell_type":"markdown","metadata":{"id":"ZVuRYtkSKzEg"},"source":["## K-means Clustering\n","\n","> The K-means clustering algorithm is an example of an iterative algorthm which tries to partition the dataset into K non-overlaping groups or clusters.\n","\n"," - `KMeans` : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_1af0PTPJey"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","# for splitting the data into train and test\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lP8-bcAjPROA"},"outputs":[],"source":["# load the iris dataset from sklearn dataset class\n","from sklearn import datasets\n","iris = datasets.load_iris()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nfqNKGcKzEh"},"outputs":[],"source":["# using train test split to divide the iris dataset into training and testing group\n","X_train, X_test, y_train, y_test = train_test_split(iris.data,\n","                                                    iris.target,\n","                                                    test_size = .3,\n","                                                    random_state = 0)\n","print('Training set has {} samples and testing set has {} samples.'\\\n","      .format(X_train.shape[0], X_test.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i24-Jab8PiEm"},"outputs":[],"source":["# specify the number of clusters\n","k = 3\n","\n","# initialize a KMeans object and fit it with train data\n","# n_init controls the number of times the algorithm runs\n","kmeans = KMeans(n_clusters = k, random_state = 0, n_init = 25).fit(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jr0GFJ0tQj0V"},"outputs":[],"source":["# make \"predictions\" on test data using the trained kmeans model\n","y_pred = kmeans.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWX31jJiQpJU"},"outputs":[],"source":["# get the centers of the 3 clusters for each feature in iris\n","pd.DataFrame(data = kmeans.cluster_centers_,\n","             columns = iris.feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKRc_JT8aC_9"},"outputs":[],"source":["# create a scatter plot where our target variable is 0\n","plt.scatter(X_test[y_pred == 0, 0], X_test[y_pred == 0, 1], s = 100,\n","            c = 'purple', label = 'Iris-setosa')\n","\n","# create a scatter plot where our target variable is 1\n","plt.scatter(X_test[y_pred == 1, 0], X_test[y_pred == 1, 1], s = 100,\n","            c = 'orange', label = 'Iris-versicolour')\n","\n","# create a scatter plot where our target variable is 2\n","plt.scatter(X_test[y_pred == 2, 0], X_test[y_pred == 2, 1], s = 100,\n","            c = 'green', label = 'Iris-virginica')\n","\n","# plot the centroids of the clusters\n","plt.scatter(kmeans.cluster_centers_[:, 0],\n","            kmeans.cluster_centers_[:,1], s = 100,\n","            c = 'red', label = 'Centroids')\n","\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"CykmhuzUautb"},"source":["## Clustering for Image Segmentation\n","\n","> Now we will use Kmeans clustering with images for Image Segmentation. Image Segmentation is a task of partitioning an image into multiple segments. For this task, we will be using a simple variation of image segmentation which is color segmentation. Color segmentation will simply assign pixels to the same segment if they have a similar color.\n","\n","> We will use `matplotlib.imread()` to load an image. The image is loaded as a 3D array, height, width, channel (3-channel for RGB or 4 -channel for RGB with alpha)."]},{"cell_type":"markdown","metadata":{"id":"WlExCydIchW3"},"source":["> Loading image into 3D array using the `imread()` function. If we look to the shape of the image, it's a 3D array. The image we loaded is a 720x1280 pixel RGB image. Therefore, the shape of the image is (720, 1280, 3)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDHv7MSsceUP"},"outputs":[],"source":["from matplotlib.image import imread\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OM-ejzqndBT1"},"outputs":[],"source":["%%bash\n","if [[ ! -f ./colored-houses.jpg ]]; then\n","    wget https://ist691.s3.amazonaws.com/images/colored-houses.jpg\n","fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bCX5LVGKzEl"},"outputs":[],"source":["image = imread('colored-houses.jpg')\n","image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OW12nh7j_2h"},"outputs":[],"source":["plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwGtpNmSSKhl"},"outputs":[],"source":["image.shape"]},{"cell_type":"markdown","metadata":{"id":"nSYgdZzNkFYw"},"source":[">We will reshape the image array as a long list of RGB colors using `reshape()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ly7YMwKrkS_k"},"outputs":[],"source":["x = image.reshape(-1,3)\n","x.shape"]},{"cell_type":"markdown","metadata":{"id":"8EUHR1cqkXZV"},"source":[">After reshapping the image, we will fit it using `KMeans` for color segmentation. Here the value of K in `KMeans` will decide the number of colors in the output image. We will try 4 different variations with cluter values of 3, 4, 5 and 8.\n","\n",">The algorithm will try to make K clusters of similar sizes. For example, it may try to find all shades of green and look for the mean color. Then it will replace all shades with the mean color."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gx6QdCljku0p"},"outputs":[],"source":["# initilize a KMeans object with 3 clusters and fitting the object with the reshaped image\n","kmeans = KMeans(n_clusters = 3).fit(x)\n","\n","# change the value of all the pixels with their cluster center value\n","segmented_img_3 = kmeans.cluster_centers_[kmeans.labels_]\n","\n","# reshape the segmentated image to the original shape\n","segmented_img_3 = segmented_img_3.reshape(image.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOYs-eDWk63x"},"outputs":[],"source":["kmeans = KMeans(n_clusters = 4).fit(x)\n","segmented_img_4 = kmeans.cluster_centers_[kmeans.labels_]\n","segmented_img_4 = segmented_img_4.reshape(image.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGOunGxgk9nj"},"outputs":[],"source":["kmeans = KMeans(n_clusters = 5).fit(x)\n","segmented_img_5 = kmeans.cluster_centers_[kmeans.labels_]\n","segmented_img_5 = segmented_img_5.reshape(image.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44hYkdkKk_QU"},"outputs":[],"source":["kmeans = KMeans(n_clusters = 8).fit(x)\n","segmented_img_6 = kmeans.cluster_centers_[kmeans.labels_]\n","segmented_img_6 = segmented_img_6.reshape(image.shape)"]},{"cell_type":"markdown","metadata":{"id":"439DhY_plEhT"},"source":[">We will now create a 2x2 grid to plot the 4 variations of color segmentation of `kmeans`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGlGK7VoKzEn"},"outputs":[],"source":["f, ((ax1,ax2), (ax3,ax4)) = plt.subplots(2, 2, sharey = True, figsize=(15,10))\n","\n","ax1.imshow((segmented_img_3).astype(np.uint8))\n","ax1.set_title('KMeans with 3 color clusters')\n","\n","ax2.imshow((segmented_img_4).astype(np.uint8))\n","ax2.set_title('KMeans with 4 color clusters')\n","\n","ax3.imshow((segmented_img_5).astype(np.uint8))\n","ax3.set_title('KMeans with 5 color clusters')\n","\n","ax4.imshow((segmented_img_6).astype(np.uint8))\n","ax4.set_title('KMeans with 8 color clusters')\n","\n","f.suptitle('Color Segmentation using KMeans with different color clusters size')\n","\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"XSfyLqg_nG4w"},"source":["> Above is the result of color segmentation. In the first picture with cluster size 3, we notice there are only three colors, a variant of white, variant of yellow, and black. But as we increase the cluster size by 1, we see a new color blue is introduced in the output. If we look at the original image we only have 6-7 houses with different variations of blue. A mean value of all blue was calculated and all the shades of blue are replaced with the blue mean.\n","\n",">As we increase the cluster size, new colors are introduced. `KMeans` tries to keep all the clusters of similar size so new colors will only be introduced when it has cluster size big enough in comparison with other color clusters."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}